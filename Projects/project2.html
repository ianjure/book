
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>EQODEC: A Carbon-Aware Deep Learning Framework for Sustainable Video Compression &#8212; Jupyter Book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Projects/project2';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="EQODEC: A Carbon-Aware Deep Learning Framework for Sustainable Video Compression" href="project2-1.html" />
    <link rel="prev" title="Simple Art Restoration with U-Net" href="project1.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.jpg" class="logo__image only-light" alt="Jupyter Book - Home"/>
    <script>document.write(`<img src="../_static/logo.jpg" class="logo__image only-dark" alt="Jupyter Book - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Welcome to my Jupyter Book!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Lectures/lecture1.html">Lecture 1</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Laboratories</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Laboratories/laboratory1.html">Laboratory 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Laboratories/laboratory2.html">Laboratory 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Laboratories/laboratory3.html">Laboratory 3</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Laboratories/laboratory4.html">Laboratory 4</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Laboratories/laboratory5.html">Laboratory 5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Laboratories/laboratory6.html">Laboratory 6</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Laboratories/laboratory7.html">Laboratory 7</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Projects</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="project1.html">Project 1</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#"><strong>EQODEC: A Carbon-Aware Deep Learning Framework for Sustainable Video Compression</strong></a></li>







</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Blogs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Blogs/blog1.html">Blog 1</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ianjure/book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ianjure/book/issues/new?title=Issue%20on%20page%20%2FProjects/project2.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/Projects/project2.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>EQODEC: A Carbon-Aware Deep Learning Framework for Sustainable Video Compression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#"><strong>EQODEC: A Carbon-Aware Deep Learning Framework for Sustainable Video Compression</strong></a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#background"><strong>Background</strong></a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#objectives"><strong>Objectives</strong></a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#methodology"><strong>Methodology</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset"><strong>Dataset</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocessing"><strong>Preprocessing</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-architecture"><strong>Model Architecture</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#carbon-aware-optimization-objective"><strong>Carbon-Aware Optimization Objective</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameters"><strong>Hyperparameters</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-procedure"><strong>Training Procedure</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-metrics"><strong>Evaluation Metrics</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#results-and-discussion"><strong>Results and Discussion</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-of-eqodec-vs-baseline"><strong>Comparison of EQODEC vs. Baseline</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#frame-reconstruction-comparison"><strong>Frame Reconstruction Comparison</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#validation-loss-curves"><strong>Validation Loss Curves</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#peak-signal-to-noise-ratio-psnr-curves"><strong>Peak Signal-to-Noise Ratio (PSNR) Curves</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#energy-efficiency-score-ees-curves"><strong>Energy-Efficiency Score (EES) Curves</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-performance-comparison"><strong>Final Performance Comparison</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion-and-reflection"><strong>Conclusion and Reflection</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion"><strong>Conclusion</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reflection"><strong>Reflection</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#references"><strong>References</strong></a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#code-repository"><strong>Code Repository</strong></a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="eqodec-a-carbon-aware-deep-learning-framework-for-sustainable-video-compression">
<h1><strong>EQODEC: A Carbon-Aware Deep Learning Framework for Sustainable Video Compression</strong><a class="headerlink" href="#eqodec-a-carbon-aware-deep-learning-framework-for-sustainable-video-compression" title="Link to this heading">#</a></h1>
<hr class="docutils" />
<p><strong>Authors:</strong> Ruszed Ayad, Ian Jure Macalisang, Honey Angel Pabololot, and Christine Joy Sorronda</p>
<p><strong>Abstract:</strong> Growing demand for online video has intensified the environmental impact of data storage and transmission, motivating environmentally sustainable compression methods. This study introduces a carbon-aware deep learning framework (EQODEC) that integrates estimated energy use and CO₂ emissions directly into the training objective of a neural video codec. The approach employs a spatiotemporal autoencoder with ConvGRU-based temporal modeling and differentiable quantization, optimized through a multi-objective loss balancing quality, bitrate, and carbon cost. Experiments on the Vimeo-90K dataset across five independent replicates demonstrate consistent sustainability gains, achieving a higher Energy Efficiency Score than a baseline model, while preserving perceptual quality and incurring no additional computational or emissions overhead.</p>
<p><strong>Keywords:</strong> Carbon-aware learning; Neural video compression; Green AI; Energy-efficient deep learning; Spatiotemporal autoencoder; Sustainability metrics; Energy Efficiency Score (EES)</p>
<br>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="background">
<h1><strong>Background</strong><a class="headerlink" href="#background" title="Link to this heading">#</a></h1>
<p>The rapid expansion of online video services has made video data the single largest contributor to global internet traffic, accounting for more than 80% of all transmitted information (Digital Age, 2023). This surge in demand has placed immense pressure on data centers, which consume massive amounts of energy to store, process, and deliver video content. As a result, the video streaming industry has become a significant source of carbon emissions, contributing to the environmental footprint of digital technologies (Afzal et al., 2024; The Shift Project, 2019).</p>
<p>Traditional codecs such as H.264 and H.265 have achieved impressive compression ratios through handcrafted engineering, yet they are approaching their practical limits, particularly as high-resolution formats (4K, 8K, VR) become mainstream (ImageKit, 2022). Recent advances in deep learning have opened new possibilities for neural video compression, where models such as autoencoders, GANs, and transformers learn to compress data by discovering efficient latent representations (Chen et al., 2024). However, most of these approaches prioritize reconstruction quality and bitrate efficiency, without explicitly considering their environmental impact (MIT News, 2025).</p>
<p>This project introduces EQODEC or the Energy-Quality Optimized Codec, a carbon-aware deep learning framework designed to make video compression both intelligent and sustainable. Unlike traditional methods, EQODEC integrates environmental cost metrics—including estimated energy use and CO₂ emissions—directly into its learning objective. By jointly optimizing for reconstruction quality, bitrate reduction, and sustainability, the model aims to reduce data center load and network energy consumption. This approach aligns with global initiatives in Green AI, promoting computational efficiency and environmental responsibility in deep learning applications (Freitag et al., 2021; Goldverg et al., 2024).</p>
<br>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="objectives">
<h1><strong>Objectives</strong><a class="headerlink" href="#objectives" title="Link to this heading">#</a></h1>
<ol class="arabic simple">
<li><p>Design a carbon-aware video compression framework that balances efficiency and environmental impact.</p></li>
<li><p>Develop an end-to-end deep learning model with an energy-aware loss to encourage eco-friendly compression.</p></li>
<li><p>Evaluate compression quality, bitrate efficiency, and carbon footprint, comparing against a baseline neural codec.</p></li>
</ol>
<br>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="methodology">
<h1><strong>Methodology</strong><a class="headerlink" href="#methodology" title="Link to this heading">#</a></h1>
<p>The EQODEC framework was developed as an environmentally conscious deep learning system for neural video compression. Its methodology integrates dataset construction, preprocessing, model architecture, carbon-aware optimization, and a comprehensive evaluation pipeline that accounts for both conventional quality metrics and sustainability-oriented measures. Unlike traditional neural compression systems that optimize solely for distortion or rate, EQODEC embeds carbon awareness directly into its training objective, enabling the model to learn representations that are simultaneously compact, high quality, and environmentally economical.</p>
<section id="dataset">
<h2><strong>Dataset</strong><a class="headerlink" href="#dataset" title="Link to this heading">#</a></h2>
<p>The experiments were conducted using the Vimeo-90K Septuplet dataset, which contains 91,701 seven-frame video sequences. Each sequence is sampled from diverse natural scenes with significant variation in illumination, motion patterns, and textures, providing an ideal foundation for learning spatiotemporal redundancies. The uniform resolution of 448×256 ensures consistent feature map alignment across sequences, simplifying batch processing and temporal modeling. The dataset is widely regarded as a standard benchmark for tasks involving frame prediction, interpolation, and compression, as its septuplet structure captures fine-grained short-range motion and realistic dynamics essential for temporal modeling in video codecs.</p>
<p>A reproducible 1% subset was selected to reduce computational cost while maintaining representative distributional characteristics. This resulted in 917 sequences, which preserved the dataset’s coverage of static scenes, dynamic shots, and camera motion.</p>
</section>
<section id="preprocessing">
<h2><strong>Preprocessing</strong><a class="headerlink" href="#preprocessing" title="Link to this heading">#</a></h2>
<p>Preprocessing began with an automated scan of the dataset directory hierarchy to ensure that each septuplet contained the full sequence of frames. All valid sequences were aggregated and randomly shuffled using a fixed seed to maintain consistency across replicates. The shuffled set was then split into training (80%), validation (10%), and test (10%) partitions, producing 733, 91, and 93 sequences respectively. The split proportions were chosen to maximize training exposure while retaining sufficient validation data for early model selection and enough test sequences to support robust statistical evaluation.</p>
<p>Each partition was recorded in a corresponding JSON index file, ensuring consistency across all training replicates and evaluation procedures. The preprocessing workflow ensured uniformity of input shapes, deterministic reproducibility, and strict separation of evaluation sequences from training data, following the documented preprocessing protocol.</p>
</section>
<section id="model-architecture">
<h2><strong>Model Architecture</strong><a class="headerlink" href="#model-architecture" title="Link to this heading">#</a></h2>
<p>The EQODEC framework is constructed upon a deep spatiotemporal autoencoder architecture designed to model both spatial structure and temporal redundancy in video sequences. The architecture integrates four primary components: a convolutional encoder, a recurrent temporal module, a differentiable quantizer, and a convolutional decoder. Each component contributes to the formation of compact, temporally coherent latent representations optimized for both reconstruction fidelity and sustainability-aware constraints.</p>
<p><strong>Convolutional Encoder</strong></p>
<p>The encoder begins by applying two convolutional layers that progressively downsample each input frame by a factor of four while simultaneously expanding the channel dimensionality. These layers operate with 5×5 kernels and stride 2, enabling the model to extract mid-level spatial abstractions such as edges, textures, and color gradients. By converting raw pixel data into higher-level spatial features, the encoder reduces redundancy and concentrates information into a compact latent feature map. ReLU activation ensures stable gradient propagation and introduces necessary nonlinearity. This spatial compression step is essential for downstream temporal modeling, as it forms a feature representation that is both efficient and semantically expressive.</p>
<p><strong>ConvGRU Temporal Modeling</strong></p>
<p>Temporal dependencies are captured through a Convolutional Gated Recurrent Unit (ConvGRU), which processes the encoded sequence frame by frame. Unlike traditional GRUs that flatten inputs, a ConvGRU preserves spatial topology by applying convolutional operations within its gating mechanisms. At each time step, the ConvGRU receives the encoded features of the current frame along with the hidden state propagated from previous frames. The reset and update gates regulate how much of the previous temporal information is retained or overwritten, while the candidate state integrates new spatial information derived from the current encoded frame. This design enables the model to encode short-term motion, temporal continuity, and structural coherence across the seven-frame sequence. As a result, the ConvGRU forms a temporally enriched latent representation that is far more compression-efficient than treating frames independently.</p>
<p><strong>Differentiable Quantization Layer</strong></p>
<p>After temporal aggregation, the resulting latent representation passes through a differentiable quantization module. To mimic the behavior of discrete quantization found in practical compression systems, the latent values are scaled and rounded during the forward pass. However, because rounding is non-differentiable, EQODEC employs a straight-through estimator (STE), which approximates the gradient during backpropagation by treating the rounding operation as the identity function. This allows the network to remain fully trainable while still being exposed to quantization effects during optimization. The quantized latent tensor serves as the effective compressed code, encouraging the model to structure its internal representation in a manner compatible with real-world bitrate constraints. This quantization stage plays a central role in bridging learned latent representations with operational compression considerations.</p>
<p><strong>Convolutional Decoder</strong></p>
<p>The decoder mirrors the encoder through two transposed convolutional layers that upsample the quantized latent tensor back to its original spatial resolution. These layers reverse the spatial reductions applied by the encoder, reconstructing fine-grained spatial details by integrating temporally coherent information supplied by the ConvGRU. The final sigmoid activation restricts pixel outputs to the normalized range [0, 1], ensuring numerically stable image formation. Although designed to be lightweight enough for efficient inference, the decoder remains expressive enough to recover high-frequency details, which are often the most vulnerable to degradation during compression.</p>
<p><strong>Architectural Coherence and Compression Suitability</strong></p>
<p>Collectively, the encoder, temporal module, quantizer, and decoder form a unified architectural system tailored to video compression. By maintaining spatial structure throughout the pipeline, leveraging recurrent modeling for temporal consistency, and incorporating quantization directly within the network, EQODEC produces latent representations that are simultaneously compact, interpretable, and aligned with the physical constraints of compression systems. Because the Baseline model employs the same architecture, all observed performance differences between the two systems can be attributed solely to the presence or absence of carbon-aware optimization rather than architectural bias.</p>
</section>
<section id="carbon-aware-optimization-objective">
<h2><strong>Carbon-Aware Optimization Objective</strong><a class="headerlink" href="#carbon-aware-optimization-objective" title="Link to this heading">#</a></h2>
<p>The EQODEC optimization objective introduces carbon-awareness into the training process through a novel multi-objective loss function:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{\text{total}}
= \lambda_\text{recon} \cdot \text{MSE}
+ \lambda_\text{rate} \cdot \text{BPP}_{\text{proxy}}
+ \lambda_\text{carbon} \cdot \text{CarbonProxy}(\mathbf{z})
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\text{MSE}\)</span> is the mean squared error between original and reconstructed frames.</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{BPP}*{\text{proxy}}\)</span> is a differentiable proxy for bits-per-pixel derived from the latent representation.</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{CarbonProxy}(\mathbf{z})\)</span> is a carbon cost surrogate weighted by the real-time carbon intensity of the compute environment.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{z}\)</span> denotes the latent tensor produced by the ConvGRU and quantizer.</p></li>
<li><p><span class="math notranslate nohighlight">\(\lambda_\text{recon}\)</span>, <span class="math notranslate nohighlight">\(\lambda_\text{rate}\)</span>, and <span class="math notranslate nohighlight">\(\lambda_\text{carbon}\)</span> are scalar hyperparameters controlling the balance between quality, bitrate, and carbon cost.</p></li>
</ul>
<p><strong>Reconstruction Loss</strong></p>
<p>The reconstruction term penalizes pixel-wise deviations between the original video frames and the reconstructed outputs. It is computed as the mean squared error across all pixels and frames. This term encourages the model to preserve visual fidelity and is heavily weighted through <span class="math notranslate nohighlight">\(\lambda_{\text{recon}} = 5.0\)</span>, ensuring that carbon- and bitrate-driven objectives do not excessively degrade image quality.</p>
<p><strong>Bitrate Proxy Loss</strong></p>
<p>The bitrate proxy term approximates bits-per-pixel through a differentiable function of the latent activations. The term is constructed using a logarithmic transformation:</p>
<div class="math notranslate nohighlight">
\[
\text{BPP}_{\text{proxy}} = \frac{1}{HW} \cdot \frac{1}{\log 2} \cdot \mathbb{E}\big[\log(1 + |\mathbf{z}|)\big]
\]</div>
<p>Where <span class="math notranslate nohighlight">\(H\)</span> and <span class="math notranslate nohighlight">\(W\)</span> denote frame height and width. This formulation reflects the intuition that higher-magnitude latent values typically imply more complex quantized codes, which translate into larger bitstreams. The term encourages representational sparsity and compactness. Weighting via <span class="math notranslate nohighlight">\(\lambda_{\text{rate}} = 0.5\)</span> balances compression efficiency against quality.</p>
<p><strong>Carbon-Aware Loss</strong></p>
<p>The carbon-aware term quantifies the environmental cost of producing the latent representation with respect to the carbon intensity of the device used for training or inference. Carbon intensity, expressed in kg CO₂/kWh, is estimated using CodeCarbon, which monitors hardware energy consumption and converts power usage into emissions estimates. The CarbonProxy function is defined as:</p>
<div class="math notranslate nohighlight">
\[
\text{CarbonProxy}(\mathbf{z}) = \alpha \cdot \mathbb{E}\big[\log(1 + |\mathbf{z}|)\big]
\]</div>
<p>Where <span class="math notranslate nohighlight">\(\alpha\)</span> is the measured carbon intensity. Since the logarithmic term parallels the bitrate proxy structure, the carbon loss effectively penalizes complex latent representations that demand more computation or energy during encoding or inference. The hyperparameter <span class="math notranslate nohighlight">\(\lambda_{\text{carbon}} = 0.005\)</span> introduces carbon-awareness without dominating the primary quality and rate objectives. The Baseline model excludes this term by setting <span class="math notranslate nohighlight">\(\lambda_{\text{carbon}} = 0\)</span>. The loss components reflect the design principles documented in the training framework and the conceptual motivation described in the EQODEC proposal.</p>
</section>
<section id="hyperparameters">
<h2><strong>Hyperparameters</strong><a class="headerlink" href="#hyperparameters" title="Link to this heading">#</a></h2>
<p>All models were trained using the Adam optimizer with a learning rate of <span class="math notranslate nohighlight">\(1 \times 10^{-4}\)</span>. Training was conducted over ten epochs with a batch size of four. The loss weights were set to <span class="math notranslate nohighlight">\(\lambda_{\text{recon}} = 5.0\)</span>, <span class="math notranslate nohighlight">\(\lambda_{\text{rate}} = 0.5\)</span>, and <span class="math notranslate nohighlight">\(\lambda_{\text{carbon}} = 0.005\)</span> for the EQODEC model. Mixed-precision training (AMP) was enabled to reduce energy consumption and memory footprint, aligning with the framework’s sustainability objectives. All hyperparameters were selected to strike a balance between performance, computational overhead, and environmental impact.</p>
</section>
<section id="training-procedure">
<h2><strong>Training Procedure</strong><a class="headerlink" href="#training-procedure" title="Link to this heading">#</a></h2>
<p>Training followed a replicate-based design in which EQODEC and the Baseline model were each trained independently across five replicates. Each replicate began with model initialization, loading of the training and validation splits, and independent weight sampling, ensuring robust statistical variance in performance metrics. Within each epoch, the training routine performed forward propagation through the encoder, ConvGRU, quantizer, and decoder, computed the multi-objective loss, and applied parameter updates via backpropagation.</p>
<p>Validation was conducted at the end of every epoch to monitor generalization performance. The model checkpoint with the lowest validation loss was saved as the best-performing version for each replicate. This approach mitigated the impact of stochastic fluctuations and ensured comparability across replicate trajectories.</p>
</section>
<section id="evaluation-metrics">
<h2><strong>Evaluation Metrics</strong><a class="headerlink" href="#evaluation-metrics" title="Link to this heading">#</a></h2>
<p>Evaluation consisted of both conventional measures of reconstruction quality and compression, as well as sustainability-centered metrics introduced by EQODEC. Each metric is detailed below.</p>
<p><strong>Peak Signal-to-Noise Ratio (PSNR)</strong></p>
<p>Reconstruction fidelity was evaluated using PSNR, computed by comparing reconstructed frames to their original counterparts. PSNR is a widely used metric in video compression, reflecting the logarithmic ratio between the maximum possible pixel value and the reconstruction error. Higher PSNR indicates better perceptual quality and lower distortion. PSNR was computed on the test split for the best model of each replicate.</p>
<p><strong>Compressed File Size</strong></p>
<p>To evaluate practical compression performance, reconstructed sequences were encoded using FFmpeg with standardized settings. The compressed size of the reconstructed video was compared to the compressed size of the original video sequence, providing a concrete measure of storage reduction. This allowed EQODEC to be evaluated not only in terms of its abstract latent representations but also in terms of its operational impact in real-world encoding workflows.</p>
<p><strong>Inference and Encoding Overhead</strong></p>
<p>The computational cost of model inference and subsequent FFmpeg encoding was recorded as part of the evaluation. This overhead reflects the time required to apply the model and produce compressed outputs. Measuring this overhead is important because environmental cost is not solely determined by the size of the data produced, but also by the computational effort required to generate it.</p>
<p><strong>CO₂ Emissions</strong></p>
<p>CodeCarbon was used to measure CO₂ emissions during inference and encoding. The tool tracked GPU utilization, runtime, and power consumption, converting these into estimates of kg CO₂ emitted. This metric provided an environmental grounding to the evaluation procedure, enabling direct comparison between compression benefits and environmental costs.</p>
<p><strong>Energy-Efficiency Score (EES)</strong></p>
<p>The Energy-Efficiency Score (EES) is the central sustainability metric introduced by EQODEC. It quantifies the amount of data saved per unit of carbon emitted:</p>
<div class="math notranslate nohighlight">
\[
\text{EES} = \frac{ \Delta S_{\text{GB}} }{ C_{\text{kg CO₂}} }
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\Delta S_{\text{GB}}\)</span> is the difference in compressed size (in gigabytes) between the original and reconstructed video.</p></li>
<li><p><span class="math notranslate nohighlight">\(C_{\text{kg CO₂}}\)</span> is the carbon overhead produced during inference and encoding.</p></li>
</ul>
<p>EES reflects the core vision of EQODEC: enabling environmentally optimized compression by directly relating compression benefits (storage savings) to environmental costs (emissions). Higher EES values indicate greater sustainability, meaning that more gigabytes are saved per unit of carbon emitted. This metric captures a dimension of model performance that traditional metrics such as PSNR or bitrate cannot represent.</p>
<br>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="results-and-discussion">
<h1><strong>Results and Discussion</strong><a class="headerlink" href="#results-and-discussion" title="Link to this heading">#</a></h1>
<p>The results of this study provide an integrated assessment of EQODEC’s performance relative to the baseline autoencoder, evaluated through sustainability-oriented metrics as well as traditional reconstruction measures. Consistent with the methodological framework, all findings are aggregated across five independent replicates to ensure robustness. The following subsections examine model outputs, training behavior, and environmental performance trends, with the aim of determining whether carbon-aware regularization can enhance energy efficiency without incurring computational or perceptual penalties.</p>
<section id="comparison-of-eqodec-vs-baseline">
<h2><strong>Comparison of EQODEC vs. Baseline</strong><a class="headerlink" href="#comparison-of-eqodec-vs-baseline" title="Link to this heading">#</a></h2>
<p>The aggregated outcomes from the five replicates demonstrate that EQODEC consistently yields modest but meaningful improvements in sustainability metrics while maintaining competitive reconstruction performance. In terms of the Energy Efficiency Score (EES), EQODEC achieves 2.1820 GB/kgCO₂, exceeding the baseline’s 2.0832 GB/kgCO₂. Although the improvement is incremental, its consistency across all runs indicates that the carbon-aware regularizer effectively encourages more storage-efficient latent representations without disrupting the stability of training.</p>
<p>With respect to reconstruction fidelity, the baseline records a higher PSNR of 17.15 dB, compared to 16.15 dB for EQODEC. This approximate 1 dB gap is aligned with expectations: while the baseline optimizes exclusively for pixel-level similarity, EQODEC balances reconstruction with latent sparsity and energy-efficient encoding. Accordingly, the modest decline in PSNR reflects an intentional trade-off rather than an indication of degraded visual performance. Later qualitative observations further confirm that EQODEC retains perceptually competitive image quality.</p>
<p>Runtime and carbon footprint remain effectively identical between both models. Evaluation times fall within the 6.9-7.0 second range for each replicate, and total CO₂ emissions differ only minimally. This parity indicates that EQODEC’s sustainability gains do not originate from increased computational expenditure. The slightly smaller compressed outputs produced by EQODEC support the idea that carbon-aware regularization encourages more compact latent codes.</p>
<p><strong>Table 1. Final Robust Comparison (N=5 Replicates)</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>METRIC</p></th>
<th class="head"><p>UNIT</p></th>
<th class="head"><p>EQODEC (Mean Std)</p></th>
<th class="head"><p>BASELINE (Mean Std)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>EES</p></td>
<td><p>GB/kgCO₂</p></td>
<td><p>2.182022 ± 0.122040</p></td>
<td><p>2.083198 ± 0.030465</p></td>
</tr>
<tr class="row-odd"><td><p>PSNR</p></td>
<td><p>dB</p></td>
<td><p>16.1505 ± 1.6436</p></td>
<td><p>17.1532 ± 0.4132</p></td>
</tr>
<tr class="row-even"><td><p>Overhead Time</p></td>
<td><p>s</p></td>
<td><p>7.002199 ± 0.168555</p></td>
<td><p>6.986921 ± 0.035673</p></td>
</tr>
<tr class="row-odd"><td><p>Compressed Size</p></td>
<td><p>GB</p></td>
<td><p>0.000366 ± 0.000045</p></td>
<td><p>0.000418 ± 0.000011</p></td>
</tr>
<tr class="row-even"><td><p>Total kgCO₂ Overhead</p></td>
<td><p>kgCO₂</p></td>
<td><p>0.000514 ± 0.000012</p></td>
<td><p>0.000513 ± 0.000003</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="frame-reconstruction-comparison">
<h2><strong>Frame Reconstruction Comparison</strong><a class="headerlink" href="#frame-reconstruction-comparison" title="Link to this heading">#</a></h2>
<p>The frame-level comparison between the original image, the baseline reconstruction, and the EQODEC output highlights perceptually meaningful differences between the two models. The baseline shows a tendency toward oversmoothing, producing blurrier textures and diminished edge detail, an expected consequence of relying solely on pixel-aligned reconstruction loss.</p>
<p>EQODEC, while still affected by the mild blur characteristic of early autoencoder architectures, demonstrates stronger preservation of structural content. Facial contours, mid-tone gradients, and local textures remain more coherent, and noise suppression is more consistent across the frame.</p>
<p>Importantly, these perceptual advantages appear despite EQODEC’s lower PSNR, underscoring the limitations of pixel-based fidelity metrics. Higher PSNR does not necessarily correspond to better visual sharpness or structural realism. EQODEC’s carbon-aware loss appears to encourage latent embeddings that retain essential spatial features while still supporting energy-efficient computation. This suggests that environmental regularization can be integrated into the training objective without sacrificing perceptual quality.</p>
<p><strong>Figure 1. Sequence Frame Comparison</strong></p>
<div style="text-align: center; margin: auto;">
    <img
      src="https://raw.githubusercontent.com/ianjure/eqodec/refs/heads/main/results/sequence_comparison.png"
      style="width: 100%; height: auto; display: block;"
    >
</div>
</section>
<section id="validation-loss-curves">
<h2><strong>Validation Loss Curves</strong><a class="headerlink" href="#validation-loss-curves" title="Link to this heading">#</a></h2>
<p>The validation loss curves exhibit smooth convergence for both models across all replicates, indicating stable and well-regularized training. The baseline maintains slightly lower validation loss throughout training, as expected given its exclusive focus on minimizing reconstruction error. EQODEC, by contrast, incorporates an additional sustainability-driven regularizer, leading to marginally higher validation loss values.</p>
<p>Nevertheless, the gap between the curves remains small and consistent, providing evidence that the carbon-aware loss term does not disrupt gradient flow or introduce instability. Both models converge reliably by Epoch 10, with no oscillations or divergence detected. This stability confirms that the ConvGRU-based architecture used in the methodology is sufficiently robust to accommodate the sustainability-aware objective.</p>
<p>The results suggest that EQODEC’s slight performance gap in standard metrics is a deliberate trade-off aligned with its design, rather than a byproduct of poor training dynamics.</p>
<p><strong>Figure 2. Validation Loss per Epoch</strong></p>
<div style="text-align: center; margin: auto;">
    <img
      src="https://raw.githubusercontent.com/ianjure/eqodec/refs/heads/main/results/val_loss_curve.png"
      style="width: 100%; height: auto; display: block;"
    >
</div>
</section>
<section id="peak-signal-to-noise-ratio-psnr-curves">
<h2><strong>Peak Signal-to-Noise Ratio (PSNR) Curves</strong><a class="headerlink" href="#peak-signal-to-noise-ratio-psnr-curves" title="Link to this heading">#</a></h2>
<p>The PSNR curves averaged across all replicates show that the baseline consistently maintains higher reconstruction fidelity throughout training, with the gap gradually widening toward later epochs. EQODEC follows a steady upward trajectory but remains approximately 1–1.5 dB below the baseline by Epoch 10.</p>
<p>Both models show smooth and monotonic PSNR progression with no evidence of instability or overfitting, reinforcing that the optimization process remained well-behaved. These trends confirm that EQODEC preserves acceptable reconstruction fidelity despite its additional sustainability constraints. The observed PSNR reduction is consistent with the methodological expectation that adding carbon-aware regularization partially shifts optimization away from pure pixel accuracy toward improved latent efficiency.</p>
<p><strong>Figure 3. PSNR per Epoch</strong></p>
<div style="text-align: center; margin: auto;">
    <img
      src="https://raw.githubusercontent.com/ianjure/eqodec/refs/heads/main/results/test_psnr_curve.png"
      style="width: 100%; height: auto; display: block;"
    >
</div>
</section>
<section id="energy-efficiency-score-ees-curves">
<h2><strong>Energy-Efficiency Score (EES) Curves</strong><a class="headerlink" href="#energy-efficiency-score-ees-curves" title="Link to this heading">#</a></h2>
<p>The EES curves reveal a consistent and increasingly apparent advantage for EQODEC as training progresses. Although both models start with similar EES values in early epochs, EQODEC’s performance improves as its latent structure stabilizes, reflecting the influence of the carbon-aware regularizer.</p>
<p>This trend is smooth, repeatable, and exhibits low variance across replicates. The use of the Vimeo-only evaluation setting further contributes to measurement stability, reducing noise that might otherwise arise from mixed-content datasets. Overall, the curves provide strong evidence that EQODEC’s design successfully encourages energy-efficient representations without compromising convergence behavior.</p>
<p><strong>Figure 4. EES per Epoch</strong></p>
<div style="text-align: center; margin: auto;">
    <img
      src="https://raw.githubusercontent.com/ianjure/eqodec/refs/heads/main/results/ees_kgco2_per_gb_curve.png"
      style="width: 100%; height: auto; display: block;"
    >
</div>
</section>
<section id="final-performance-comparison">
<h2><strong>Final Performance Comparison</strong><a class="headerlink" href="#final-performance-comparison" title="Link to this heading">#</a></h2>
<p>The replicate-level bar-chart comparison reinforces the consistency of EQODEC’s improvements. Across all runs, EQODEC achieves a measurable advantage in Energy Efficiency Score, supported by small error bars that indicate stable and repeatable environmental performance.</p>
<p>Although the baseline achieves a higher PSNR by approximately one decibel, this is consistent with the epoch-level trends and reflects the expected behavior of a model optimized solely for reconstruction quality. Carbon emissions for both models remain nearly identical, highlighting that EQODEC’s sustainability improvements stem from more efficient latent representations rather than reduced computational load.</p>
<p><strong>Figure 5. EQODEC vs. Baseline</strong></p>
<div style="text-align: center; margin: auto;">
    <img
      src="https://raw.githubusercontent.com/ianjure/eqodec/refs/heads/main/results/bar_chart.png"
      style="width: 100%; height: auto; display: block;"
    >
</div>
<p>Taken together, these findings position EQODEC as an effective carbon-aware video compression model. It consistently improves sustainability metrics, particularly EES and compressed size, without introducing additional runtime or emissions. Despite a modest reduction in PSNR, qualitative evaluations show that EQODEC maintains competitive visual quality, sometimes exceeding the baseline in perceptual coherence. The stability of training across all replicates substantiates the feasibility of integrating carbon-awareness directly into the learning objective.</p>
<p>Overall, the combined results from PSNR analysis, EES progression, validation curves, and replicate comparisons demonstrate that EQODEC reliably enhances carbon efficiency while maintaining acceptable reconstruction fidelity. This establishes a compelling foundation for further exploration of sustainability-aware neural compression models.</p>
<br>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="conclusion-and-reflection">
<h1><strong>Conclusion and Reflection</strong><a class="headerlink" href="#conclusion-and-reflection" title="Link to this heading">#</a></h1>
<section id="conclusion">
<h2><strong>Conclusion</strong><a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>This study introduced EQODEC, a carbon-aware neural video compression framework designed to bridge the gap between compression efficiency and environmental sustainability. By integrating a novel carbon-aware loss term into the training objective, EQODEC jointly optimizes for reconstruction quality, bitrate reduction, and estimated carbon emissions. Evaluated on the Vimeo-90K dataset across five independent replicates, the framework demonstrated consistent and measurable improvements in sustainability metrics without substantially compromising perceptual quality.</p>
<p>The results confirm that EQODEC successfully enhances Energy Efficiency Score (EES)—achieving 2.1820 GB/kgCO₂ compared to the baseline’s 2.0832 GB/kgCO₂—indicating more storage saved per unit of carbon emitted. This improvement is attributed to the carbon-aware regularizer, which encourages more compact and efficient latent representations within the spatiotemporal autoencoder architecture. While the baseline model attained a higher Peak Signal-to-Noise Ratio (PSNR) by approximately 1 dB, qualitative assessment of reconstructed frames revealed that EQODEC preserved perceptually important structural details and edge coherence, underscoring that pixel-level metrics alone do not fully capture visual quality.</p>
<p>Critically, these sustainability gains were achieved without increasing computational overhead; inference time and total CO₂ emissions during evaluation remained nearly identical between EQODEC and the baseline. This confirms that the framework’s benefits stem from learned representational efficiency rather than from reductions in runtime or energy use during inference.</p>
<p>In summary, EQODEC establishes a viable pathway for incorporating carbon-awareness directly into neural video compression. It provides a practical methodology for developing deep learning systems that align with Green AI principles, demonstrating that environmental impact can be optimized alongside traditional compression objectives.</p>
</section>
<section id="reflection">
<h2><strong>Reflection</strong><a class="headerlink" href="#reflection" title="Link to this heading">#</a></h2>
<p>Developing EQODEC underscored the challenge of translating environmental sustainability into a concrete and trainable deep learning objective. A central difficulty was designing a carbon-aware loss that meaningfully influenced optimization while remaining stable and compatible with standard compression objectives. Balancing reconstruction quality, bitrate efficiency, and carbon estimation required careful calibration to ensure that sustainability considerations shaped the learned representations without overwhelming perceptual fidelity or convergence behavior.</p>
<p>This work also prompted a re-evaluation of how compression performance is assessed. Conventional metrics such as PSNR and bitrate, while important, proved insufficient for capturing sustainability-oriented improvements. The introduction of the Energy Efficiency Score (EES) reflects a broader shift toward evaluating efficiency in terms of environmental cost, highlighting that meaningful progress in Green AI requires expanding beyond purely quality-driven benchmarks. This reframing challenges established evaluation norms and suggests the need for sustainability-aware metrics to become more widely adopted in neural compression research.</p>
<p>Finally, the results demonstrate that environmental considerations can be integrated without increasing computational overhead or compromising practicality. EQODEC achieved improved sustainability outcomes through representational efficiency rather than reduced runtime or energy usage during inference, reinforcing the idea that greener models need not be slower or more expensive to deploy. Nonetheless, the reliance on proxy-based carbon estimation remains a limitation, pointing to future work involving more precise, hardware- and location-aware measurements, as well as broader validation across datasets and modalities.</p>
<br>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="references">
<h1><strong>References</strong><a class="headerlink" href="#references" title="Link to this heading">#</a></h1>
<p>Afzal, S., et al. (2024). A Survey on Energy Consumption and Environmental Impact of Video Streaming. <a class="reference external" href="https://arxiv.org/abs/2401.09854">https://arxiv.org/abs/2401.09854</a></p>
<p>Chen, J., et al. (2024). Deep Compression Autoencoder for Efficient High-Resolution Diffusion Models. <a class="reference external" href="https://arxiv.org/abs/2410.10733">https://arxiv.org/abs/2410.10733</a></p>
<p>Digital Age. (2023). How does digital video consumption contribute to the	global carbon footprint? <a class="reference external" href="https://newdigitalage.co/general/how-does-digital-video-consumption-contribute-to-the-global-carbon-footprint/">https://newdigitalage.co/general/how-does-digital-video-consumption-contribute-to-the-global-carbon-footprint/</a></p>
<p>Freitag, C., et al. (2021). The climate impact of ICT: A review of estimates, trends and regulations. <a class="reference external" href="https://arxiv.org/abs/2102.02622">https://arxiv.org/abs/2102.02622</a></p>
<p>Goldverg,	J.,	et	al.	(2024).	Carbon-Aware	End-to-End	Data	Movement. <a class="reference external" href="https://arxiv.org/abs/2406.09650">https://arxiv.org/abs/2406.09650</a></p>
<p>ImageKit. (2022). H.264 Vs. H.265: An analytical breakdown of video streaming codecs. <a class="reference external" href="https://imagekit.io/blog/h264-vs-h265/">https://imagekit.io/blog/h264-vs-h265/</a></p>
<p>MIT	News.	(2025).	Explained:	Generative	AI’s	environmental	impact. <a class="reference external" href="https://news.mit.edu/2025/explained-generative-ai-environmental-impact-0117">https://news.mit.edu/2025/explained-generative-ai-environmental-impact-0117</a></p>
<p>The	Shift	Project.	(2019).	The Unsustainable Use of Online Video. <a class="reference external" href="https://theshiftproject.org/app/uploads/2025/04/Press-kit_Climate-crisis_The-unsustainable-use-of-online-video.pdf">https://theshiftproject.org/app/uploads/2025/04/Press-kit_Climate-crisis_The-unsustainable-use-of-online-video.pdf</a></p>
<br>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="code-repository">
<h1><strong>Code Repository</strong><a class="headerlink" href="#code-repository" title="Link to this heading">#</a></h1>
<p>Link: <a class="github reference external" href="https://github.com/ianjure/eqodec">ianjure/eqodec</a></p>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./Projects"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="project1.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><strong>Simple Art Restoration with U-Net</strong></p>
      </div>
    </a>
    <a class="right-next"
       href="project2-1.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><strong>EQODEC: A Carbon-Aware Deep Learning Framework for Sustainable Video Compression</strong></p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#"><strong>EQODEC: A Carbon-Aware Deep Learning Framework for Sustainable Video Compression</strong></a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#background"><strong>Background</strong></a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#objectives"><strong>Objectives</strong></a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#methodology"><strong>Methodology</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset"><strong>Dataset</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocessing"><strong>Preprocessing</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-architecture"><strong>Model Architecture</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#carbon-aware-optimization-objective"><strong>Carbon-Aware Optimization Objective</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameters"><strong>Hyperparameters</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-procedure"><strong>Training Procedure</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-metrics"><strong>Evaluation Metrics</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#results-and-discussion"><strong>Results and Discussion</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-of-eqodec-vs-baseline"><strong>Comparison of EQODEC vs. Baseline</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#frame-reconstruction-comparison"><strong>Frame Reconstruction Comparison</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#validation-loss-curves"><strong>Validation Loss Curves</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#peak-signal-to-noise-ratio-psnr-curves"><strong>Peak Signal-to-Noise Ratio (PSNR) Curves</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#energy-efficiency-score-ees-curves"><strong>Energy-Efficiency Score (EES) Curves</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-performance-comparison"><strong>Final Performance Comparison</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion-and-reflection"><strong>Conclusion and Reflection</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion"><strong>Conclusion</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reflection"><strong>Reflection</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#references"><strong>References</strong></a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#code-repository"><strong>Code Repository</strong></a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Ian Jure Macalisang
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>